{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file trains the random forest models and creates the reconstructions. Both are saved to directories and can be found on this project's figshare page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# For accessing directories\n",
    "# =========================================\n",
    "root_dir = \"/local/data/artemis/workspace/jfs2167/recon_eval\" # Set this to the path of the project\n",
    "\n",
    "reference_output_dir = f\"{root_dir}/references\"\n",
    "data_output_dir = f\"{root_dir}/data/processed\"\n",
    "model_output_dir = f\"{root_dir}/models/trained\"\n",
    "recon_output_dir = f\"{root_dir}/models/reconstructions\"\n",
    "other_output_dir = f\"{root_dir}/models/performance_metrics\"\n",
    "\n",
    "# =========================================\n",
    "# Number of cores you have access to for model training\n",
    "# =========================================\n",
    "jobs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# machine learning libraries\n",
    "import sklearn            # machine-learning libary with many algorithms implemented\n",
    "from sklearn.ensemble import RandomForestRegressor # random forest regressor (RFR)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Python file with supporting functions\n",
    "import pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predefined values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading references\n",
    "path_LET = f\"{reference_output_dir}/members_LET_dict.pickle\"\n",
    "path_seeds = f\"{reference_output_dir}/random_seeds.npy\"\n",
    "path_loc = f\"{reference_output_dir}/members_seed_loc_dict.pickle\"\n",
    "\n",
    "with open(path_LET,'rb') as handle:\n",
    "    mems_dict = pickle.load(handle)\n",
    "    \n",
    "random_seeds = np.load(path_seeds)    \n",
    "    \n",
    "with open(path_loc,'rb') as handle:\n",
    "    seed_loc_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Defining some inputs for the modeling process\n",
    "# =========================================\n",
    "\n",
    "# Train-validate-test split proportions\n",
    "val_prop = .2\n",
    "test_prop = .2\n",
    "\n",
    "# Parameter grids\n",
    "rf_param_grid = {'n_estimators': [50,100, 200],\n",
    "                 'max_depth': [30,40]\n",
    "                }\n",
    "\n",
    "# Feature and target lists for feeding into the models\n",
    "features_sel = ['sst_detrend', 'sst_anom', 'sss', 'sss_anom', 'mld_clim_log', 'chl_log', 'chl_anom', 'xco2', 'A', 'B', 'C', 'T0', 'T1']\n",
    "target_sel = ['pCO2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approach_output_dir = f\"{other_output_dir}/{approach}\"\n",
    "param_fname = f\"{approach_output_dir}/{approach}_best_params_dict.pickle\"\n",
    "\n",
    "with open(param_fname, 'rb') as handle:\n",
    "    best_params = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data, mask it, train/val/test split, run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = {} # Uncomment if running cross validation to find best params\n",
    "test_performance = defaultdict(dict)\n",
    "unseen_performance = defaultdict(dict)\n",
    "\n",
    "K_folds = 3\n",
    "approach = \"rf\"\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "for ens, mem_list in mems_dict.items():\n",
    "    print(ens)\n",
    "    first_mem = False # Set to true if you want to tune parameters with the first member from each ensemble\n",
    "    for member in mem_list:\n",
    "        print(member)\n",
    "        seed_loc = seed_loc_dict[ens][member] # Figure out which column to use for seeds\n",
    "        \n",
    "        # Data file path\n",
    "        data_dir = f\"{data_output_dir}/{ens}/member_{member}\"\n",
    "        fname = f\"data_clean_2D_mon_{ens}_{member}_1x1_198201-201701.pkl\"\n",
    "        file_path = f\"{data_dir}/{fname}\"\n",
    "        \n",
    "        # Read in data, create some selection filters, produce a reduced dataframe\n",
    "        df = pd.read_pickle(file_path)\n",
    "        # 816 represents 3 standard deviations above mean in SOCAT data -- observations above this threshold are unrealistic to observe in real-world data\n",
    "        recon_sel = (~df[features_sel+target_sel+['net_mask']].isna().any(axis=1)) & (df[target_sel] < 816).to_numpy().ravel()\n",
    "        sel = (recon_sel & (df['socat_mask'] == 1))\n",
    "        unseen_sel = (recon_sel & (df['socat_mask'] == 0))\n",
    "        \n",
    "        # Convert dataframe to numpy arrays, train/val/test split\n",
    "        X = df.loc[sel,features_sel].to_numpy()\n",
    "        y = df.loc[sel,target_sel].to_numpy().ravel()\n",
    "                \n",
    "        N = X.shape[0]\n",
    "        train_val_idx, train_idx, val_idx, test_idx = pre.train_val_test_split(N, test_prop, val_prop, random_seeds, seed_loc)\n",
    "        X_train_val, X_train, X_val, X_test, y_train_val, y_train, y_val, y_test = pre.apply_splits(X, y, train_val_idx, train_idx, val_idx, test_idx) \n",
    "        \n",
    "        # Define the model based on which approach to use    \n",
    "        if first_mem:\n",
    "            model = RandomForestRegressor(random_state=random_seeds[2,seed_loc], n_jobs=jobs)\n",
    "            param_grid = rf_param_grid\n",
    "            grid = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=K_folds, return_train_score=False, refit=False)\n",
    "            grid.fit(X_train_val, y_train_val)\n",
    "            best_params[ens] = grid.best_params_\n",
    "            first_mem = False\n",
    "\n",
    "        # Fit the model on train/validation data\n",
    "        model = RandomForestRegressor(random_state=random_seeds[3,seed_loc], **best_params[ens], n_jobs=jobs)\n",
    "        model.fit(X_train_val, y_train_val)          \n",
    "\n",
    "        # Save the model\n",
    "        pre.save_model(model, model_output_dir, approach, ens, member)\n",
    "\n",
    "        # Calculate some test error metrics and store in a dictionary\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        test_performance[ens][member] = pre.evaluate_test(y_test, y_pred_test)\n",
    "        \n",
    "        # Redo this analysis on the unseen data\n",
    "        y_pred_unseen = model.predict(df.loc[unseen_sel,features_sel].to_numpy())\n",
    "        y_unseen = df.loc[unseen_sel,target_sel].to_numpy().ravel()\n",
    "        unseen_performance[ens][member] = pre.evaluate_test(y_unseen, y_pred_unseen)\n",
    "\n",
    "        # Create the reconstruction and save it\n",
    "        y_pred_seen = model.predict(X)\n",
    "        df['pCO2_recon'] = np.nan\n",
    "        df.loc[unseen_sel,['pCO2_recon']] = y_pred_unseen\n",
    "        df.loc[sel,['pCO2_recon']] = y_pred_seen\n",
    "        DS_recon = df[['net_mask','socat_mask','pCO2', 'pCO2_recon']].to_xarray()\n",
    "        pre.save_recon(DS_recon, recon_output_dir, approach, ens, member)\n",
    "\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best parameters and performance metrics\n",
    "approach_output_dir = f\"{other_output_dir}/{approach}\"\n",
    "param_fname = f\"{approach_output_dir}/{approach}_best_params_dict.pickle\"\n",
    "test_perform_fname = f\"{approach_output_dir}/{approach}_test_performance_dict.pickle\"\n",
    "unseen_perform_fname = f\"{approach_output_dir}/{approach}_unseen_performance_dict.pickle\"\n",
    "\n",
    "Path(approach_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(param_fname, 'wb') as handle:\n",
    "    pickle.dump(best_params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(test_perform_fname, 'wb') as handle:\n",
    "    pickle.dump(test_performance, handle)\n",
    "with open(unseen_perform_fname, 'wb') as handle:\n",
    "    pickle.dump(unseen_performance, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert performance metrics to dataframes\n",
    "test_df = pd.DataFrame.from_dict({(i,j): test_performance[i][j]\n",
    "                                  for i in test_performance.keys()\n",
    "                                  for j in test_performance[i].keys()},\n",
    "                                 orient='index')\n",
    "\n",
    "unseen_df = pd.DataFrame.from_dict({(i,j): unseen_performance[i][j]\n",
    "                                  for i in unseen_performance.keys()\n",
    "                                  for j in unseen_performance[i].keys()},\n",
    "                                 orient='index')\n",
    "\n",
    "test_df.index.names = [\"model\",\"member\"]\n",
    "unseen_df.index.names = [\"model\",\"member\"]\n",
    "\n",
    "# Save the dataframes too\n",
    "test_df_fname = f\"{approach_output_dir}/{approach}_test_performance_df.pickle\"\n",
    "unseen_df_fname = f\"{approach_output_dir}/{approach}_unseen_performance_df.pickle\"\n",
    "\n",
    "test_df.to_pickle(test_df_fname)\n",
    "unseen_df.to_pickle(unseen_df_fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (jfs_devd)",
   "language": "python",
   "name": "jfs_devd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
