{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file trains the feed forward neural network models and creates the reconstructions. Both are saved to directories and can be found on this project's figshare page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# For accessing directories\n",
    "# =========================================\n",
    "root_dir = \"/local/data/artemis/workspace/jfs2167/recon_eval\" # Set this to the path of the project\n",
    "\n",
    "reference_output_dir = f\"{root_dir}/references\"\n",
    "data_output_dir = f\"{root_dir}/data/processed\"\n",
    "model_output_dir = f\"{root_dir}/models/trained\"\n",
    "recon_output_dir = f\"{root_dir}/models/reconstructions\"\n",
    "other_output_dir = f\"{root_dir}/models/performance_metrics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# machine learning libraries\n",
    "import sklearn            # machine-learning libary with many algorithms implemented\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# Python file with supporting functions\n",
    "import pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predefined values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading references\n",
    "path_LET = f\"{reference_output_dir}/members_LET_dict.pickle\"\n",
    "path_seeds = f\"{reference_output_dir}/random_seeds.npy\"\n",
    "path_loc = f\"{reference_output_dir}/members_seed_loc_dict.pickle\"\n",
    "\n",
    "with open(path_LET,'rb') as handle:\n",
    "    mems_dict = pickle.load(handle)\n",
    "    \n",
    "random_seeds = np.load(path_seeds)    \n",
    "    \n",
    "with open(path_loc,'rb') as handle:\n",
    "    seed_loc_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Defining some inputs for the modeling process\n",
    "# =========================================\n",
    "\n",
    "# Train-validate-test split proportions\n",
    "val_prop = .2\n",
    "test_prop = .2\n",
    "\n",
    "# Feature and target lists for feeding into the models\n",
    "features_sel = ['sst_detrend', 'sst_anom', 'sss', 'sss_anom', 'mld_clim_log', 'chl_log', 'chl_anom', 'xco2', 'A', 'B', 'C', 'T0', 'T1']\n",
    "target_sel = ['pCO2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data, mask it, train/val/test split, run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = defaultdict(dict) # We train 5 models per member and keep track of validation set performance to determine which to pick as the best\n",
    "test_dict = defaultdict(dict)\n",
    "unseen_dict = defaultdict(dict)\n",
    "\n",
    "for ens, members in mems_dict.items():\n",
    "    for member in members:\n",
    "        val_dict[ens][member] = {}\n",
    "        test_dict[ens][member] = {}\n",
    "        unseen_dict[ens][member] = {}\n",
    "\n",
    "approach = \"nn\"\n",
    "num_runs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "for ens, mem_list in mems_dict.items():\n",
    "    print(ens)\n",
    "    for member in mem_list:\n",
    "        print(member)\n",
    "        seed_loc = seed_loc_dict[ens][member] # Figure out which column to use for seeds\n",
    "        \n",
    "        # Data file path\n",
    "        data_dir = f\"{data_output_dir}/{ens}/member_{member}\"\n",
    "        fname = f\"data_clean_2D_mon_{ens}_{member}_1x1_198201-201701.pkl\"\n",
    "        file_path = f\"{data_dir}/{fname}\"\n",
    "        \n",
    "        # Read in data, create some selection filters, produce a reduced dataframe\n",
    "        df = pd.read_pickle(file_path)\n",
    "        # 816 represents 3 standard deviations above mean in SOCAT data -- observations above this threshold are unrealistic to observe in real-world data\n",
    "        recon_sel = (~df[features_sel+target_sel+['net_mask']].isna().any(axis=1)) & (df[target_sel] < 816).to_numpy().ravel()\n",
    "        sel = (recon_sel & (df['socat_mask'] == 1))\n",
    "        unseen_sel = (recon_sel & (df['socat_mask'] == 0))\n",
    "        \n",
    "        # Convert dataframe to numpy arrays, train/val/test split\n",
    "        X = df.loc[sel,features_sel].to_numpy()\n",
    "        y = df.loc[sel,target_sel].to_numpy().ravel()\n",
    "                \n",
    "        N = X.shape[0]\n",
    "        train_val_idx, train_idx, val_idx, test_idx = pre.train_val_test_split(N, test_prop, val_prop, random_seeds, seed_loc)\n",
    "        X_train_val, X_train, X_val, X_test, y_train_val, y_train, y_val, y_test = pre.apply_splits(X, y, train_val_idx, train_idx, val_idx, test_idx) \n",
    "        \n",
    "        X_unseen = df.loc[unseen_sel,features_sel].to_numpy()\n",
    "        y_unseen = df.loc[unseen_sel,target_sel].to_numpy().ravel()\n",
    "        \n",
    "        #Standardize data based on the training data\n",
    "        X_train_s = (X_train - np.mean(X_train,axis=0))/np.std(X_train,axis=0)\n",
    "        X_val_s = (X_val - np.mean(X_train,axis=0))/np.std(X_train,axis=0)\n",
    "        X_test_s = (X_test - np.mean(X_train,axis=0))/np.std(X_train,axis=0)\n",
    "        X_unseen_s = (X_unseen - np.mean(X_train,axis=0))/np.std(X_train,axis=0)\n",
    "        X_s = (X - np.mean(X_train,axis=0))/np.std(X_train,axis=0)\n",
    "        \n",
    "        # Fit the model on train data\n",
    "        for i in range(num_runs):\n",
    "            print(i)\n",
    "            \n",
    "            models = pre.build_nn_vf(num_features=len(features_sel))\n",
    "            models.fit(X_train_s, y_train, epochs=200, batch_size=1000, verbose=0)\n",
    "\n",
    "            y_pred_val = models.predict(X_val_s).ravel()\n",
    "            y_pred_test = models.predict(X_test_s).ravel()\n",
    "            y_pred_unseen = models.predict(X_unseen_s, batch_size=int(1e6)).ravel()\n",
    "            y_pred_seen = models.predict(X_s, batch_size=int(1e5)).ravel()\n",
    "\n",
    "            # update this function to handle multiple runs for NN\n",
    "            pre.save_model(models, model_output_dir, approach, ens, member, run=i)\n",
    "            \n",
    "            val_dict[ens][member][i] = pre.evaluate_test(y_val,y_pred_val)\n",
    "            test_dict[ens][member][i] = pre.evaluate_test(y_test,y_pred_test)\n",
    "            unseen_dict[ens][member][i] = pre.evaluate_test(y_unseen,y_pred_unseen)  \n",
    "\n",
    "            # Create the reconstruction and save it\n",
    "            df['pCO2_recon'] = np.nan\n",
    "            df.loc[unseen_sel,['pCO2_recon']] = y_pred_unseen\n",
    "            df.loc[sel,['pCO2_recon']] = y_pred_seen\n",
    "            DS_recon = df[['net_mask','socat_mask','pCO2', 'pCO2_recon']].to_xarray()\n",
    "            \n",
    "            # update this function to handle multiple runs for NN\n",
    "            pre.save_recon(DS_recon, recon_output_dir, approach, ens, member, run=i)\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ens, members in val_dict.items():\n",
    "    for mem,runs in members.items():\n",
    "        if runs:\n",
    "            min_bias_idx = min(runs.items(), key=lambda x: np.abs(x[1]['bias']))[0]\n",
    "            min_mse_idx = min(runs.items(), key=lambda x: np.abs(x[1]['mse']))[0]\n",
    "            mse_threshold = sorted([value['mse'] for key,value in runs.items()])[1]\n",
    "            options = [(key,value['bias']) for key,value in runs.items() if value['mse'] <= mse_threshold]\n",
    "            min_bias_mse_idx = sorted(options, key=lambda x: np.abs(x[1]))[0][0]\n",
    "\n",
    "            val_dict[ens][mem][min_bias_idx]['sel_min_bias'] = 1\n",
    "            val_dict[ens][mem][min_mse_idx]['sel_min_mse'] = 1\n",
    "            val_dict[ens][mem][min_bias_mse_idx]['sel_min_bias_mse'] = 1\n",
    "\n",
    "            test_dict[ens][mem][min_bias_idx]['sel_min_bias'] = 1\n",
    "            test_dict[ens][mem][min_mse_idx]['sel_min_mse'] = 1\n",
    "            test_dict[ens][mem][min_bias_mse_idx]['sel_min_bias_mse'] = 1\n",
    "\n",
    "            unseen_dict[ens][mem][min_bias_idx]['sel_min_bias'] = 1\n",
    "            unseen_dict[ens][mem][min_mse_idx]['sel_min_mse'] = 1\n",
    "            unseen_dict[ens][mem][min_bias_mse_idx]['sel_min_bias_mse'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionaries to pandas data frames\n",
    "val_df = pd.DataFrame.from_dict({(i,j,k):val_dict[i][j][k]\n",
    "                                 for i in val_dict.keys()\n",
    "                                 for j in val_dict[i].keys()\n",
    "                                 for k in val_dict[i][j].keys()},\n",
    "                                orient=\"index\")\n",
    "\n",
    "test_df = pd.DataFrame.from_dict({(i,j,k):test_dict[i][j][k]\n",
    "                                  for i in test_dict.keys()\n",
    "                                  for j in test_dict[i].keys()\n",
    "                                  for k in test_dict[i][j].keys()},\n",
    "                                 orient=\"index\")\n",
    "\n",
    "unseen_df = pd.DataFrame.from_dict({(i,j,k):unseen_dict[i][j][k]\n",
    "                                    for i in unseen_dict.keys()\n",
    "                                    for j in unseen_dict[i].keys()\n",
    "                                    for k in unseen_dict[i][j].keys()},\n",
    "                                   orient=\"index\")\n",
    "\n",
    "val_df.index.names = [\"model\", \"member\", \"run\"]\n",
    "test_df.index.names = [\"model\", \"member\", \"run\"]\n",
    "unseen_df.index.names = [\"model\", \"member\", \"run\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best parameters and performance metrics\n",
    "approach_output_dir = f\"{other_output_dir}/{approach}\"\n",
    "\n",
    "val_dict_fname = f\"{approach_output_dir}/{approach}_val_performance_dict.pickle\"\n",
    "test_dict_fname = f\"{approach_output_dir}/{approach}_test_performance_dict.pickle\"\n",
    "unseen_dict_fname = f\"{approach_output_dir}/{approach}_unseen_performance_dict.pickle\"\n",
    "\n",
    "val_df_fname = f\"{approach_output_dir}/{approach}_val_performance_df.pickle\"\n",
    "test_df_fname = f\"{approach_output_dir}/{approach}_test_performance_df.pickle\"\n",
    "unseen_df_fname = f\"{approach_output_dir}/{approach}_unseen_performance_df.pickle\"\n",
    "\n",
    "Path(approach_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(val_dict_fname, 'wb') as handle:\n",
    "    pickle.dump(val_dict, handle)\n",
    "with open(test_dict_fname, 'wb') as handle:\n",
    "    pickle.dump(test_dict, handle)\n",
    "with open(unseen_dict_fname, 'wb') as handle:\n",
    "    pickle.dump(unseen_dict, handle)\n",
    "    \n",
    "val_df.to_pickle(val_df_fname)\n",
    "test_df.to_pickle(test_df_fname)\n",
    "unseen_df.to_pickle(unseen_df_fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (jfs_devd)",
   "language": "python",
   "name": "jfs_devd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
